# Cloud Data Engineering â€” End-to-End ETL Pipeline (Python, Airflow, dbt & PostgreSQL)

Este projeto demonstra a construÃ§Ã£o de um **pipeline de dados completo em ambiente cloud**, cobrindo **ingestÃ£o, orquestraÃ§Ã£o, transformaÃ§Ã£o, modelagem e qualidade de dados**, utilizando ferramentas amplamente adotadas em ambientes produtivos de **Data Engineering**.

O pipeline foi desenhado para simular um cenÃ¡rio real de engenharia de dados, indo alÃ©m de data warehouses gerenciados, com foco em **infraestrutura, automaÃ§Ã£o, versionamento e observabilidade**.

---

## ğŸ¯ Objetivo do Projeto

- Construir um pipeline **end-to-end** desde a extraÃ§Ã£o de dados brutos atÃ© camadas analÃ­ticas.
- Trabalhar com **dados externos via API**, lidando com falhas, schemas instÃ¡veis e dados incompletos.
- Implementar **orquestraÃ§Ã£o automatizada** com controle de dependÃªncias, retries e execuÃ§Ã£o incremental.
- Aplicar **boas prÃ¡ticas modernas de transformaÃ§Ã£o de dados** com dbt.
- Utilizar **arquitetura em camadas (Medallion Architecture)** fora de um data warehouse gerenciado.
- Simular um ambiente prÃ³ximo ao **mundo real de Data Engineering em cloud**.

---

## ğŸ§± Stack TecnolÃ³gica

### â˜ï¸ Infraestrutura & Cloud
- **Cloud Provider:** DigitalOcean  
- **Ambiente:** Linux VM (Droplet)
- **ContainerizaÃ§Ã£o:** Docker & Docker Compose

**Por quÃª?**  
Permite aprender conceitos reais de infraestrutura, rede, processos e deploy, que sÃ£o abstraÃ­dos em soluÃ§Ãµes totalmente gerenciadas.

---

### ğŸ—„ï¸ Banco de Dados
- **PostgreSQL**

Utilizado como:
- Camada **Raw (Bronze)** para dados brutos ingeridos
- Base para transformaÃ§Ã£o e modelagem analÃ­tica

---

### ğŸ”Œ IngestÃ£o de Dados
- **Python**
  - `requests` / `aiohttp`
  - `pandas` (uso leve)
- **Fonte:** API pÃºblica (dados reais e nÃ£o tratados)

**Responsabilidade:**
- ExtraÃ§Ã£o dos dados
- NormalizaÃ§Ã£o mÃ­nima de schema
- PersistÃªncia dos dados brutos sem regras de negÃ³cio

---

### ğŸ”„ OrquestraÃ§Ã£o
- **Apache Airflow**

**Responsabilidade:**
- Orquestrar todo o pipeline de ponta a ponta
- Controlar dependÃªncias entre etapas
- Implementar:
  - retries
  - scheduling
  - backfill
  - logs e monitoramento

---

### ğŸ§ª TransformaÃ§Ã£o & Modelagem
- **dbt Core**

**Responsabilidade:**
- TransformaÃ§Ã£o dos dados usando SQL versionado
- ImplementaÃ§Ã£o da **Medallion Architecture**
- AplicaÃ§Ã£o de regras de negÃ³cio
- Garantia de qualidade via testes automatizados

Camadas:
- **Bronze:** dados brutos espelhados
- **Silver:** dados limpos, padronizados e deduplicados
- **Gold:** dados modelados para analytics e BI

---

### ğŸ“ˆ Consumo AnalÃ­tico
- Camada **Gold** pronta para:
  - BI (Power BI / Tableau)
  - AnÃ¡lises exploratÃ³rias
  - RelatÃ³rios executivos

---

### âš™ï¸ Dev Experience & Qualidade
- **GitHub**
- **GitHub Actions (CI)**
- **GitHub Copilot**
- **JetBrains DataGrip / PyCharm**

**Responsabilidade:**
- Versionamento de cÃ³digo
- AutomaÃ§Ã£o de validaÃ§Ãµes (lint, dbt tests)
- PadronizaÃ§Ã£o do ambiente de desenvolvimento

---

## ğŸ”„ Arquitetura do Pipeline


